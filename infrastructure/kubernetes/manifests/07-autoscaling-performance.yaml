# ==============================================
# CRYB PLATFORM - AUTO-SCALING & PERFORMANCE
# ==============================================
# Advanced auto-scaling with KEDA, VPA, and
# performance optimization for millions of users
# ==============================================

---
# Vertical Pod Autoscaler for API
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: cryb-api-vpa
  namespace: cryb-api
  labels:
    app: cryb-api
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cryb-api
  updatePolicy:
    updateMode: "Auto"
    minReplicas: 6
  resourcePolicy:
    containerPolicies:
      - containerName: cryb-api
        minAllowed:
          cpu: 250m
          memory: 512Mi
        maxAllowed:
          cpu: 4000m
          memory: 8Gi
        controlledResources: ["cpu", "memory"]
        controlledValues: RequestsAndLimits

---
# KEDA ScaledObject for Advanced Auto-scaling
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: cryb-api-scaledobject
  namespace: cryb-api
  labels:
    app: cryb-api
spec:
  scaleTargetRef:
    name: cryb-api
  pollingInterval: 15
  cooldownPeriod: 300
  idleReplicaCount: 6
  minReplicaCount: 6
  maxReplicaCount: 200
  fallback:
    failureThreshold: 3
    replicas: 10
  advanced:
    restoreToOriginalReplicaCount: false
    horizontalPodAutoscalerConfig:
      name: cryb-api-hpa-keda
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 20
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 60
          policies:
            - type: Percent
              value: 100
              periodSeconds: 30
            - type: Pods
              value: 10
              periodSeconds: 30
          selectPolicy: Max
  triggers:
    # CPU-based scaling
    - type: cpu
      metricType: Utilization
      metadata:
        value: "70"
    
    # Memory-based scaling
    - type: memory
      metricType: Utilization
      metadata:
        value: "80"
    
    # Prometheus-based scaling (active connections)
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.cryb-monitoring.svc.cluster.local:9090
        metricName: active_connections_per_pod
        threshold: "1000"
        query: sum(rate(socket_io_connected_total{job="cryb-api"}[2m])) by (pod)
    
    # Prometheus-based scaling (request rate)
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.cryb-monitoring.svc.cluster.local:9090
        metricName: http_requests_per_second
        threshold: "500"
        query: sum(rate(http_requests_total{job="cryb-api"}[2m])) by (pod)
    
    # Redis queue depth
    - type: redis
      metadata:
        address: cryb-redis-primary.cryb-database.svc.cluster.local:6379
        password: REDIS_PASSWORD
        listName: message_queue
        listLength: "100"
    
    # PostgreSQL connection count
    - type: postgresql
      metadata:
        connection: postgresql://cryb_admin:PASSWORD@cryb-postgres-primary.cryb-database.svc.cluster.local:5432/cryb
        query: SELECT sum(numbackends) FROM pg_stat_database WHERE datname='cryb'
        targetQueryValue: "80"

---
# ScaledObject for Real-time Services
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: cryb-realtime-scaledobject
  namespace: cryb-realtime
  labels:
    app: cryb-realtime
spec:
  scaleTargetRef:
    name: livekit
  pollingInterval: 10
  cooldownPeriod: 180
  idleReplicaCount: 2
  minReplicaCount: 2
  maxReplicaCount: 50
  triggers:
    # Active voice/video sessions
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.cryb-monitoring.svc.cluster.local:9090
        metricName: active_sessions_per_pod
        threshold: "50"
        query: sum(livekit_room_participants_total{job="livekit"}) by (pod)
    
    # CPU for real-time workloads
    - type: cpu
      metricType: Utilization
      metadata:
        value: "60"  # Lower threshold for real-time services

---
# ScaledObject for Database Services
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: cryb-postgres-replica-scaledobject
  namespace: cryb-database
  labels:
    app: cryb-postgres
    role: replica
spec:
  scaleTargetRef:
    name: cryb-postgres-replica
  pollingInterval: 30
  cooldownPeriod: 600
  idleReplicaCount: 2
  minReplicaCount: 2
  maxReplicaCount: 8
  triggers:
    # Database connection count
    - type: postgresql
      metadata:
        connection: postgresql://cryb_admin:PASSWORD@cryb-postgres-primary.cryb-database.svc.cluster.local:5432/cryb
        query: SELECT sum(numbackends) FROM pg_stat_database WHERE datname='cryb'
        targetQueryValue: "150"
    
    # Read query rate
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.cryb-monitoring.svc.cluster.local:9090
        metricName: postgres_read_queries_per_second
        threshold: "1000"
        query: sum(rate(pg_stat_user_tables_seq_scan_total{datname="cryb"}[5m])) + sum(rate(pg_stat_user_tables_idx_scan_total{datname="cryb"}[5m]))

---
# Cluster Autoscaler Configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
    tier: infrastructure
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
        tier: infrastructure
    spec:
      serviceAccountName: cluster-autoscaler
      priorityClassName: system-cluster-critical
      tolerations:
        - operator: Exists
          effect: NoSchedule
      nodeSelector:
        kubernetes.io/os: linux
      
      containers:
        - name: cluster-autoscaler
          image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.28.0
          args:
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/cryb-production-primary
            - --balance-similar-node-groups
            - --scale-down-enabled=true
            - --scale-down-delay-after-add=10m
            - --scale-down-unneeded-time=10m
            - --scale-down-delay-after-delete=10s
            - --scale-down-delay-after-failure=3m
            - --scale-down-utilization-threshold=0.5
            - --skip-nodes-with-system-pods=false
            - --max-node-provision-time=15m
          
          env:
            - name: AWS_REGION
              value: us-east-1
          
          resources:
            requests:
              memory: "300Mi"
              cpu: "100m"
            limits:
              memory: "600Mi"
              cpu: "500m"
          
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 65534
          
          livenessProbe:
            httpGet:
              path: /health-check
              port: 8085
            initialDelaySeconds: 600
            periodSeconds: 60
          
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
      
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"

---
# Descheduler for Optimal Pod Placement
apiVersion: batch/v1
kind: CronJob
metadata:
  name: descheduler
  namespace: kube-system
  labels:
    app: descheduler
    tier: infrastructure
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: descheduler
          restartPolicy: Never
          priorityClassName: system-cluster-critical
          
          containers:
            - name: descheduler
              image: k8s.gcr.io/descheduler/descheduler:v0.27.1
              args:
                - --policy-config-file=/policy-dir/policy.yaml
                - --v=3
                - --dry-run=false
              
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "100m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"
              
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
                readOnlyRootFilesystem: true
                runAsNonRoot: true
                runAsUser: 65534
              
              volumeMounts:
                - name: policy-volume
                  mountPath: /policy-dir
          
          volumes:
            - name: policy-volume
              configMap:
                name: descheduler-policy

---
# Node Problem Detector
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-problem-detector
  namespace: kube-system
  labels:
    app: node-problem-detector
    tier: infrastructure
spec:
  selector:
    matchLabels:
      app: node-problem-detector
  template:
    metadata:
      labels:
        app: node-problem-detector
        tier: infrastructure
    spec:
      serviceAccountName: node-problem-detector
      hostNetwork: true
      tolerations:
        - operator: Exists
          effect: NoSchedule
      
      containers:
        - name: node-problem-detector
          image: k8s.gcr.io/node-problem-detector/node-problem-detector:v0.8.13
          args:
            - --logtostderr
            - --config.system-log-monitor=/config/kernel-monitor.json,/config/docker-monitor.json,/config/systemd-monitor.json
            - --config.system-stats-monitor=/config/system-stats-monitor.json
            - --config.custom-plugin-monitor=/config/network-problem-monitor.json
          
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          
          securityContext:
            privileged: true
          
          volumeMounts:
            - name: log
              mountPath: /var/log
              readOnly: true
            - name: kmsg
              mountPath: /dev/kmsg
              readOnly: true
            - name: localtime
              mountPath: /etc/localtime
              readOnly: true
            - name: config
              mountPath: /config
              readOnly: true
      
      volumes:
        - name: log
          hostPath:
            path: /var/log/
        - name: kmsg
          hostPath:
            path: /dev/kmsg
        - name: localtime
          hostPath:
            path: /etc/localtime
        - name: config
          configMap:
            name: node-problem-detector-config

---
# Performance Tuning DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: performance-tuner
  namespace: kube-system
  labels:
    app: performance-tuner
    tier: infrastructure
spec:
  selector:
    matchLabels:
      app: performance-tuner
  template:
    metadata:
      labels:
        app: performance-tuner
        tier: infrastructure
    spec:
      serviceAccountName: performance-tuner
      hostNetwork: true
      hostPID: true
      tolerations:
        - operator: Exists
          effect: NoSchedule
      
      initContainers:
        - name: sysctl-tuner
          image: busybox:1.35
          command: ['sh', '-c']
          args:
            - |
              # Network performance tuning
              sysctl -w net.core.somaxconn=65535
              sysctl -w net.core.netdev_max_backlog=5000
              sysctl -w net.core.rmem_max=134217728
              sysctl -w net.core.wmem_max=134217728
              sysctl -w net.ipv4.tcp_rmem="4096 65536 134217728"
              sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
              sysctl -w net.ipv4.tcp_congestion_control=bbr
              sysctl -w net.ipv4.tcp_slow_start_after_idle=0
              sysctl -w net.ipv4.tcp_tw_reuse=1
              
              # Memory management
              sysctl -w vm.max_map_count=262144
              sysctl -w vm.swappiness=1
              sysctl -w vm.dirty_ratio=15
              sysctl -w vm.dirty_background_ratio=5
              
              # File system
              sysctl -w fs.file-max=2097152
              sysctl -w fs.inotify.max_user_watches=1048576
              
              echo "System tuning completed"
          securityContext:
            privileged: true
          volumeMounts:
            - name: proc
              mountPath: /host/proc
            - name: sys
              mountPath: /host/sys
      
      containers:
        - name: performance-monitor
          image: busybox:1.35
          command: ['sh', '-c', 'while true; do sleep 3600; done']
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 65534
      
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys

---
# Service Accounts
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/CrybClusterAutoscalerServiceRole

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: descheduler
  namespace: kube-system
  labels:
    app: descheduler

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-problem-detector
  namespace: kube-system
  labels:
    app: node-problem-detector

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: performance-tuner
  namespace: kube-system
  labels:
    app: performance-tuner

---
# Priority Classes
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: cryb-high-priority
value: 1000
globalDefault: false
description: "High priority class for critical CRYB services"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: cryb-medium-priority
value: 500
globalDefault: false
description: "Medium priority class for standard CRYB services"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: cryb-low-priority
value: 100
globalDefault: false
description: "Low priority class for background CRYB services"

---
# Performance Testing Job
apiVersion: batch/v1
kind: Job
metadata:
  name: performance-test
  namespace: cryb-api
  labels:
    app: performance-test
    tier: testing
spec:
  parallelism: 10
  completions: 10
  backoffLimit: 3
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: k6-load-test
          image: grafana/k6:latest
          args:
            - run
            - --vus=100
            - --duration=5m
            - --rps=1000
            - /scripts/load-test.js
          
          env:
            - name: API_URL
              value: "https://api.cryb.ai"
            - name: WEBSOCKET_URL
              value: "wss://api.cryb.ai/socket.io"
          
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          
          volumeMounts:
            - name: test-scripts
              mountPath: /scripts
      
      volumes:
        - name: test-scripts
          configMap:
            name: k6-test-scripts